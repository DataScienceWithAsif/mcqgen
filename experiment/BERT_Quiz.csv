MCQ,Choices,correct_Ans
What is the purpose of the Next Sentence Prediction (NSP) task in BERT training?,a: To predict the next word in a sentence | b: To determine if two sentences have similar meanings | c: To classify images in a dataset | d: To generate new text based on input,b
How is the NSP loss calculated in BERT training?,a: Based on the length of the input sentences | b: Based on the model's ability to distinguish true pairs from random pairs | c: Based on the number of layers in the model | d: Based on the learning rate of the optimizer,b
What tokens are introduced to the input representation in BERT for NSP?,a: [CLS] and [SEP] | b: [START] and [END] | c: [TOKEN] and [MASK] | d: [BEGIN] and [STOP],a
"In BERT training, what does the output vector from the final layer associated with the [CLS] token represent?",a: The first sentence in the pair | b: The second sentence in the pair | c: The next sentence prediction | d: The loss function value,c
What percentage of training pairs in BERT consists of positive pairs for NSP?,a: 25% | b: 50% | c: 75% | d: 100%,b
