Training Bidirectional Encoders (NSP)
 • An important class of applications involves determining the relationship between
 pairs of sentences– paraphrase detection (detecting if two sentences have similar meanings)– entailment (detecting if the meanings of two sentences entail or contradict each other)– discourse coherence (deciding if two neighboring sentences form a coherent discourse)
 • To capture the kind of knowledge required for applications such as these, BERT
 introduced a second learning objective called Next Sentence Prediction (NSP)
 • Training: The model is presented with pairs of sentences and is asked to predict
 whether each pair consists of an actual pair of adjacent sentences from the training
 corpus or a pair of unrelated sentences.
 • In BERT, 50% of the training pairs consisted of positive pairs, and in the other 50%
 the second sentence of a pair was randomly selected from elsewhere in the corpus– TheNSPloss is based on how well the model can distinguish true pairs from random pairs
 • BERTintroduces two new tokens to the input representation– After tokenizing the input with the subword model, the token [CLS] is prepended to the
 input sentence pair, and the token [SEP] is placed between the sentences and after the
 final token of the second sentence– During training, the output vector from the final layer associated with the [CLS] token
 represents the next sentence prediction
